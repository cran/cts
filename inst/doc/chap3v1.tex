

\subsection{CAR model}
Suppose we have data $x_{\tau}$ observed on time $t_{\tau}$ for $\tau=1,2, ...,n$. We assume noise-affected observations $x_{\tau}=y(t_{\tau})+\eta_{\tau}$ and $y(t_{\tau})$ follows a $p$-th order continuous time autoregressive process $Y(t)$ satisfying
\begin{align}\label{E:ARp}
     Y^{(p)}(t)+\alpha_1Y^{(p-1)}(t)+\ldots+\alpha_{p-1}Y^{(1)}(t)+\alpha_pY(t)=\epsilon(t),
\end{align}
where $Y^{(i)}(t)$ is the $i$th derivative of $Y(t)$ and $\epsilon(t)$ is the formal derivative of a Brownian process $B(t)$ with variance parameter $\sigma^2 = \textrm{var}\{B(t +1) - B(t)\}$.
In addition, it will be assumed that $\eta_{\tau}$ is a normally distributed random variable representing observational error, uncorrelated with $\epsilon(t)$, and $
E(\eta_\tau) =0;
E(\eta_j \eta_k) =0, \mbox{for } j\neq k; 
E(\eta_\tau^2) =\gamma\sigma^2.$

The operator notation of model (\ref{E:ARp}) is $\alpha(D)Y(t)=\epsilon(t)$ where
\begin{equation}
\alpha(D)=D^p+\alpha_1 D^{p-1}+...+\alpha_{p-1} D+\alpha_{p},
\end{equation}
where $D$ is the derivative operator. The corresponding characteristic equation is then given by
\begin{equation}
\alpha(s)=s^p+\alpha_1 s^{p-1}+...+\alpha_{p-1} s+\alpha_{s}=0.
\end{equation}
To assure the stability of the model, a parameterization 
was constructed on the zeros $r_1,...,r_p$ of $\alpha(s)$ \citep{Jone:1981}, i.e., 
\begin{equation}\label{E:carfac}
\alpha(s)=\prod_{i=1}^{p}(s-r_i).
\end{equation}


The model in the \pkg{cts} package follows the reparameterization \citep{Belc:Hamp:Tunn:1994}:
\begin{equation}\label{E:belc1}
\alpha(D)Y(t)=(1+D/\kappa)^{p-1}\epsilon(t),
\end{equation}
with scaling parameter $\kappa>0$. This introduces a prescribed moving average operator of order $p-1$ into the model, which makes the model selection convenient along with other theoretic benefits described in \citet{Belc:Hamp:Tunn:1994}. 

The power spectrum of the $p$th order continuous process (\ref{E:belc1}) is defined by
\begin{equation}\label{E:carspe}
G_y(f)=\sigma^2\left|\frac{(1+\mbox{i}2\pi f/\kappa)^{p-1}}{\alpha(\mbox{i}2\pi f)}\right|^2.
\end{equation}
The system frequencies are determined by the roots of (\ref{E:carfac}). In fact, the representation of (\ref{E:carfac}) breaks a $p$th order autoregressive operator into its irreducible first and quadratic factors that have complex roots. A quadratic factor $(s-r_{2k-1})(s-r_{2k})$ with complex poles is associated with ``cyclic'' behavior in data, given at $f=\frac{|\Im(r_{2k})|}{2\pi}$, where $|\Im(r_{2k})|$ is the absolute value of the imaginary part of $r_{2k}$. Actually, the true cyclic behavior with these cycles is present in the autocorrelations and only approximately present in the realization itself. For a first order factor with the pole $r_k$, the system frequency is $f=0$ if $r_k<0$, and $f=0.5$ if $r_k>0$, which corresponds to a white noise process.


\subsection{Kalman filtering}\label{sub:modest}

This section deals with the details related to applying the Kalman filter to estimate the parameters of model~(\ref{E:belc1}), following \citet{Jone:1981} and \citet{Belc:Hamp:Tunn:1994}.
 To apply the Kalman filter, it is required to rewrite model~(\ref{E:belc1}) to a state space form, which may be found in \citet{Wibe:1971}. %Let $A$ and $R$ be defined as in (\ref{E:Amat}) and (\ref{E:Rvec}), respectively, and 
Let the unobservable state vector $\theta(t)=(z(t), z'(t), ...,z^{(p-1)}(t))^T$. The state equation is then given by
\begin{equation}\label{E:carsta}
\theta'=A\theta+R\epsilon,
\end{equation}
where
\begin{equation}\label{E:Amat}
A= \begin{bmatrix}
            0 & 1 & \ldots & 0  \\
            0 & 0 & \ldots & 0  \\
            \vdots \\
            0 & 0 & \ldots & 1 \\
            -\alpha_p & -\alpha_{p-1} & \ldots & -\alpha_1\\
          \end{bmatrix}
\end{equation}
and
\begin{equation}\label{E:Rvec}
 R'=         \begin{bmatrix}
            0 & 0 & \hdots & 1\\
          \end{bmatrix}.
\end{equation}
The observation equation is given by
\begin{equation}\label{E:carobs}
x_{\tau}=H\theta(t_{\tau})+\eta_{\tau},
\end{equation}
where the elements of the $1\times p$ vector $H$ are given by
\begin{equation}\label{E:Hvec}
H_i={p-1 \choose i-1} /\kappa^{i-1} \qquad i=1,...,p.
\end{equation}
Suppose that $A$ can be diagonalized by $A=U\bold{D}U^{-1}$, where
\begin{equation}\label{E:Umat}
U= \begin{bmatrix}
            1 & 1 & \ldots & 1  \\
            r_1 & r_2 & \ldots & r_p  \\
            r^2_1 & r^2_2 & \ldots & r^2_p \\
            \vdots \\
            r^{p-1}_1 & r^{p-1}_2 & \ldots & r^{p-1}_p\\
          \end{bmatrix},
\end{equation}
$r_1, r_2, ..., r_p$ are the roots of $\alpha(s)$, and $\bold{D}$ is a diagonal matrix with these roots as its diagonal elements. In this case, we let $\theta=U\psi$, and the state equation becomes
\begin{equation}\label{E:stadiag}
\psi'=\bold{D}\psi+J\epsilon,
\end{equation}
where $J=U^{-1}R$. Consequently, the observation equation becomes 
\begin{equation}
x_{\tau}=G\psi(t_{\tau})+\eta_{\tau}
\end{equation}
where $G=HU$. The necessary and sufficient condition for the diagonalization of $A$ is that $A$ has distinct eigenvalues. %While the standard form provides reliable numerical solution, 
The diagonal form not only provides computational efficiency, but also provides an interpretation of unobserved components.
 The evaluation of $T_{\theta,\tau}=e^{A\delta_\tau}$ (standard form) is required where $\delta_\tau=t_\tau-t_{\tau-1}$. % as given before.
 For a review of computations related to the exponential of a matrix, see \citet{Mole:Loan:2003}. For the diagonal form, $T_{\psi,\tau}=e^{\bold{D}\delta_\tau}$ is diagonal with elements $e^{r_i\delta_\tau}$. When a diagonal form is not available, a numerical matrix exponential evaluation is needed.

To start the Kalman filter recursions, initial conditions are in demand. For a stationary model, the unconditional covariance matrix of state vector $\theta(t)$ is known \citep{Doob:1953} and used in \citet{Jone:1981} and \citet[\S9.1]{Harv:1990}. The initial state for both standard and diagonalized version can be set as $\theta_0=0$ and $\psi_0=0$, respectively. The stationary covariance matrix $Q$ satisfies
\begin{equation}\label{E:Q}
Q=\sigma^2\int_{0}^{\infty}e^{As}RR'e^{A's}ds.
\end{equation}

When $A$ can be diagonalized, it is straightforward to show that
\begin{equation}\label{E:Qini}
Q_{\psi_{i,j}}=-\sigma^2J_i \bar{J_j}/(r_i+\bar{r}_j),
\end{equation}
where $\bar{J_j}$ and $\bar{r}_j$ are complex conjugates of $J_j$ and $r_j$, respectively.

 The scale parameter $\kappa$ can be chosen approximately as the reciprocal of the mean time between observations. The algorithm of Kalman filter for the diagonal form is presented below. Starting with an initial stationary state vector of $\psi_0=\psi(0|0)=0$ and the initial stationary state covariance matrix $Q_{\psi}$ (\ref{E:Qini}), the recursion proceeds as follows:
   \begin{enumerate}\label{equ:diagpre}
    \item  Predict the state. Let
    \begin{equation}\label{E:carkal1d}
    T_{\psi,\tau}=e^{\bold{D}\tau}
    \end{equation}
    a diagonal matrix, then
    \begin{equation}
    \psi(t_k|t_{k-1})=T_{\psi,\tau}\psi(t_{k-1}|t_{k-1}).
    \end{equation}
    \item Calculate the covariance matrix of this prediction:  
\begin{align}
P_\psi(t_k|t_{k-1}) %&=T_{\psi,\tau}P_\psi(t_{k-1}|t_{k-1})\bar{T}_{\psi,\tau}+Q_\psi(t_k|t_{k-1}) \notag\\
=&T_{\psi,\tau}(P_\psi(t_{k-1}|t_{k-1})-Q_\psi)\bar{T}_{\psi,\tau} +Q_\psi.
 \end{align}
    \item Predict the observation at time $t_k$:
    \begin{equation}\label{E:carkal4d}
    x_\psi(t_k|t_{k-1})=G\psi(t_k|t_{k-1})
    \end{equation}
    \item Calculate the innovation:
    \begin{equation}
    v_\psi(t_k)=x_\psi(t_k)-x_\psi(t_k|t_{k-1})
    \end{equation}
    and variance
     \begin{equation}
     F_\psi(t_k)=GP_\psi(t_k|t_{k-1})\bar{G}^{'}+V
     \end{equation}
    \item Update the estimate of the state vector:
     \begin{equation}
     \psi(t_k|t_k)=\psi(t_k|t_{k-1})+P_\psi(t_k|t_{k-1})\bar{G}'F_\psi^{-1}(t_k)v_\psi(t_k)
     \end{equation}
    \item Update the covariance matrix:
    \begin{equation}
    P_\psi(t_k|t_k)=P_\psi(t_k|t_{k-1})-P_\psi(t_k|t_{k-1})\bar{G}'F_\psi^{-1}(t_k)G\bar{P}'_\psi(t_k|t_{k-1})
    \end{equation}
    \item %As for the standard form,
 The unknown scale factor $\sigma^2$ can be concentrated out by letting $\sigma^2=1$ temporally. %Hence again from (\ref{E:loglike}), 
-2 log-likelihood is calculated by
    \begin{equation}\label{E:loglikecd}
    \log L_{\psi,c}=\sum_{t=1}^{n}\log F_\psi(t_k)+n\log \sum_{t=1}^{n}v_\psi^2(t_k)/F_\psi(t_k)
    \end{equation}
    The log-likelihood function (\ref{E:loglikecd}) thus can be evaluated by a recursive application of the Kalman filter, and a nonlinear numerical optimization routine is then used to determine the parameter estimation. 
The unknown scale factor can then be estimated by
    \begin{equation}
    \hat{\sigma}^2=\frac{1}{n}\sum_{t=1}^{n}v_\psi^2(t_k)/F_\psi(t_k).
    \end{equation}
    \end{enumerate}
When a diagonal form is not stable, a standard form Kalman filter recursion may be found in \citet{Belc:Hamp:Tunn:1994} or \citet{Wang:2004}.
However the computational load is reduced dramatically with the diagonal form since matrix $\bold{D}$ is diagonal. 

When the nonlinear optimization is successfully completed, in addition to the maximum likelihood estimation of the parameters and error variances, the Kalman filter returns the optimal estimate of the state and the state covariance matrix at the last time point. The forecasting of the state, state covariance matrix and observation can be continued into future desired time points using equations from (\ref{E:carkal1d}) to (\ref{E:carkal4d}).
\subsection{Model selection}\label{S:caraic}

To identify a model order, \citet{Belc:Hamp:Tunn:1994} proposed a strategy corresponding to the reparameterization. Start with a large order model, and obtain the parameter vector $\phi$ and its covariance matrix $V_\phi$, we then make a Cholesky decomposition such that $V^{-1}_\phi=L_\phi L'_\phi$ where $L_\phi$ is a lower triangular matrix, and define the vector $t_\phi=L'_\phi \phi$ and construct the sequence $AIC_k=-\sum_{i=1}^{k}t^2_{\phi,i}+2k$. The index of the minimum value of $AIC_k$ suggests a preferred model order. In addition, if the true model order $p$ is less than the large value used for model estimation, then for $i>p$ the $t$-statistics may be treated as normal-distributed variables, so that the deviation from their true values of 0 will be small. 

\subsection{Diagnostics}\label{S:cardiag}

The assumptions underlying the model (\ref{E:carsta}) and (\ref{E:carobs}) are that the disturbances $\epsilon(t)$ and $\eta_{\tau}$ are normally distributed and serially independent with constant variances. Based on these assumptions, the standardized one-step forecast errors
\begin{equation}
e(t_k)=v(t_k)/\sqrt{F(t_k)} \qquad k=1,...,n
\end{equation}
are also normally distributed and serially independent with unit variance. Hence, in addition to inspection of time plot, the QQ-normal plot can be used to visualize the ordered residuals against their theoretical quantiles. For a
white noise sequence, the sample autocorrelations are approximately independently
and normally distributed with zero means and variances $1/n$. Note that for a purely random series, the cumulative periodogram should follow along a line $y=2x$ where $x$ is frequency. 
A standard portmanteau test statistic for serial correlation, such as the Ljung-Box statistic, can be used as well.

\subsection{Kalman smoothing}\label{S:carsmo}

For a structural time series model, it is often of interest to estimate the unobserved components at all points in the sample. Estimation of smoothed trend and cyclical components provides an example. %As discussed in Section~\ref{S:smo}, 
The purpose of smoothing at time $t$ is to find the expected value of the state vector, conditional on the information made available after time $t$.
 In this section, a fixed-interval smoothing algorithm \citep[\S3.6.2]{Harv:1990} is implemented with modifications for the model considered, though a more efficient approach is possible, see the discussion in \citet[\S4.3]{Durb:Koop:2001}. Estimating unobserved components relies on the diagonal form which provides the associated structure with the corresponding roots $r_1, ... r_p$. 
The smoothing state and covariance matrix are given by
\begin{align}\label{equ:smoscm} 
\psi_s(t_k|t_n)&=\psi(t_k|t_k)+P^*(t_k)(\psi_s(t_{k+1}|t_n)%-\psi(t_{k+1}|t_k))
-\psi(t_{k+1}|t_k))\\
P_s(t_k|t_n)&=P(t_k|t_k)+P^*(t_k)(P_s(t_{k+1}|t_n)%-P(t_{k+1}|t_k))\bar{P}^*(t_k)\\
-P(t_{k+1}|t_k))\bar{P}^*(t_k)\\
\intertext{where} 
P^*(t_k)&=P(t_k|t_k)\bar{T}_{\psi,\tau+1}P^{-1}(t_{k+1}|t_k)
\end{align}
and ${T}_{\psi,\tau+1}=e^{\bold{D}(t_{k+2}-t_{k+1})}$, and $\bar{T}_{\psi,\tau+1}$ and $\bar{P}(t_k|t_k)$ are complex conjugates. To start the recursion, the initial values are given by $\psi_s(t_n|t_n)=\psi(t_n|t_n)$ and $P_s(t_n|t_n)=P(t_n|t_n)$. The observed value $x_{\tau}$, in the absence of measurement error, is the sum of contributions from the diagonalized state variables $\psi$, i.e., $x_{\tau}=\sum_jG_j\psi_j(t_{\tau})$. Therefore, the original data may be partitioned, as in \citet[\S7.3.5]{Jenk:Watt:1968}. Any pair of two complex conjugate zeros of (\ref{E:carfac}) is associated with two corresponding state variables whose combined contribution to $x_{\tau}$ represents a source of diurnal variation. One possible real zero contributes a low frequency component and the other possible real zero contributes a white noise component. Hence, the contributions $G_j\psi_j$ at every time point can be estimated from all the data using the Kalman smoother as described above.



